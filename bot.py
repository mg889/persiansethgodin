import os
import re
import feedparser
from bs4 import BeautifulSoup
from telegram import Bot
from transformers import MarianMTModel, MarianTokenizer
import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ---------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ----------
TOKEN = os.getenv("BOT_TOKEN")       # Ø§Ø² GitHub Secrets Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒØ´Ù‡
CHANNEL_ID = os.getenv("CHANNEL_ID") # Ù…Ø«Ù„Ø§ "@persiansethgodin"
LAST_FILE = "last_post.txt"
MODEL_NAME = "Helsinki-NLP/opus-mt-en-fa"  # Ù…Ø¯Ù„ ØªØ±Ø¬Ù…Ù‡ en->fa

# ---------- Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… ----------
bot = Bot(token=TOKEN)

# ---------- ØªØ§Ø¨Ø¹ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ† (Ø­Ø°Ù HTMLØŒ URLØŒ Ú©Ø¯Ù‡Ø§ÛŒ ØºÙ„Ø· Ùˆ Ø§Ø¹Ø¯Ø§Ø¯) ----------
def clean_text(html_text):
    # Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ Ùˆ entity Ù‡Ø§ÛŒ HTML
    soup = BeautifulSoup(html_text or "", "html.parser")
    text = soup.get_text(separator=" ")

    # Ø­Ø°Ù Ú©Ø¯Ù‡Ø§ÛŒ numeric entity Ù…Ø«Ù„ &#8217;
    text = re.sub(r'&#\d+;', ' ', text)

    # Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
    text = re.sub(r'http\S+', ' ', text)

    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ Ùˆ amp;
    text = text.replace("amp;", " ")
    text = re.sub(r'&[a-zA-Z]+;', ' ', text)

    # Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ (Ø§Ú¯Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ù‡Ù…Ù‡ Ø§Ø¹Ø¯Ø§Ø¯ Ø­Ø°Ù Ù†Ø´Ù†ØŒ Ø®Ø· Ø²ÛŒØ± Ø±Ùˆ ØªØºÛŒÛŒØ± Ø¨Ø¯Ù‡)
    text = re.sub(r'\d+', ' ', text)

    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ---------- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ ØªØ±Ø¬Ù…Ù‡ (ÛŒÚ©â€ŒØ¨Ø§Ø± Ø¯Ø± Ø´Ø±ÙˆØ¹) ----------
def load_translation_model(model_name=MODEL_NAME):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Loading tokenizer & model ({model_name}) on {device} ...")
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(model_name).to(device)
    logger.info("Model loaded.")
    return tokenizer, model, device

tokenizer, model, device = load_translation_model()

# ---------- ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ† Ø¨Ø§ ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ ØªÚ©Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ø±Ø´ Ù†Ø§Ø®ÙˆØ§Ø³ØªÙ‡ ----------
def translate_text(text):
    if not text:
        return ""

    # ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª (ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹)
    sentences = re.split(r'(?<=[.!?])\s+', text)

    chunks = []
    cur = ""
    for s in sentences:
        # Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„ Ú©Ø§Ø±Ø§Ú©ØªØ± ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (ØªÙ‚Ø±ÛŒØ¨ÛŒØ› ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ù‡Ù… Ø¯Ø± Ù…Ø¯Ù„ Ú©Ù†ØªØ±Ù„ Ù…ÛŒØ´Ù†)
        if not cur:
            cur = s
        elif len(cur) + len(s) < 1000:  # Ø­Ø¯ÙˆØ¯ÛŒØ› Ø§Ú¯Ø± Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ø¨Ù„Ù†Ø¯Ù‡ Ø§ÛŒÙ† Ø¹Ø¯Ø¯ Ø±Ùˆ Ú©Ù…/Ø²ÛŒØ§Ø¯ Ú©Ù†
            cur += " " + s
        else:
            chunks.append(cur)
            cur = s
    if cur:
        chunks.append(cur)

    translations = []
    for chunk in chunks:
        # encode
        inputs = tokenizer(chunk, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
        translated_tokens = model.generate(**inputs, max_length=512)
        out = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
        translations.append(out)

    return " ".join(translations).strip()

# ---------- Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ (ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª Ø­Ø¯Ø§Ú©Ø«Ø± ~4000 Ú©Ø§Ø±Ø§Ú©ØªØ±) ----------
def send_long_message(bot, chat_id, text):
    max_len = 4000
    parts = [text[i:i+max_len] for i in range(0, len(text), max_len)]
    for p in parts:
        bot.send_message(chat_id=chat_id, text=p)

# ---------- Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø®ÙˆØ§Ù†Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ù„ÛŒÙ†Ú© ----------
def get_last_post_id():
    if os.path.exists(LAST_FILE):
        with open(LAST_FILE, "r", encoding="utf-8") as f:
            return f.read().strip()
    return None

def save_last_post_id(post_id):
    with open(LAST_FILE, "w", encoding="utf-8") as f:
        f.write(post_id or "")

# ---------- Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ ----------
def send_latest_post():
    logger.info("Fetching feed...")
    feed_url = "https://seths.blog/feed"
    feed = feedparser.parse(feed_url)

    if not feed.entries:
        logger.warning("No feed entries found.")
        return

    entry = feed.entries[0]
    title = entry.title if "title" in entry else "(Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†)"
    link = entry.link if "link" in entry else ""
    summary_raw = entry.summary if "summary" in entry else entry.get("content", [{}])[0].get("value", "")

    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
    summary_clean = clean_text(summary_raw)

    # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø¢Ø®Ø±ÛŒÙ† Ù„ÛŒÙ†Ú© Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡
    last = get_last_post_id()
    if last == link:
        logger.info("No new post (same link). Exiting.")
        return

    # ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ†
    logger.info("Translating summary to Persian...")
    try:
        summary_fa = translate_text(summary_clean)
    except Exception as e:
        logger.exception("Translation failed, falling back to cleaned English summary.")
        summary_fa = ""  # ÛŒØ§ Ù…ÛŒâ€ŒØ´Ù‡ summary_clean Ø±Ùˆ Ø¨Ø°Ø§Ø±ÛŒÙ…

    # Ø³Ø§Ø®Øª Ù¾ÛŒØ§Ù… (Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ + ÙØ§Ø±Ø³ÛŒ)
    # Ú©ÙˆØªØ§Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ø³Ø®Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø±Ø§ÛŒ Ø²ÛŒØ¨Ø§ÛŒÛŒ (Ù…Ø«Ù„Ø§Ù‹ 1000 Ú©Ø§Ø±Ø§Ú©ØªØ±)
    en_preview = summary_clean if len(summary_clean) <= 1000 else summary_clean[:1000].rstrip() + "..."

    message = f"ğŸ“Œ {title}\n\nğŸ‡¬ğŸ‡§ {en_preview}\n\nğŸ‡®ğŸ‡· {summary_fa}\n\nğŸ”— Ù…Ù†Ø¨Ø¹: {link}"
    logger.info("Sending message to channel...")
    send_long_message(bot, CHANNEL_ID, message)
    logger.info("Message sent. Saving last_post...")
    save_last_post_id(link)

if __name__ == "__main__":
    send_latest_post()
    # Ø°Ø®ÛŒØ±Ù‡ Ø¢Ø®Ø±ÛŒÙ† Ù„ÛŒÙ†Ú©
    save_last_post_id(link)

if __name__ == "__main__":
    send_latest_post()
